{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "Due by 10/12/15 at 11:59pm EST\n",
    "\n",
    "-----------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# special IPython command to prepare the notebook for matplotlib\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "## Start working 10/1/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 Regression (20 pts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1a \n",
    "\n",
    "Compare different models for the regressionP1.txt file where you are to model output given combinations of the x1, x2, and x3 variables and find the best model.  Note that you can include not only the individual variables but the 2- and 3-way interactions that use the format x1:x2 or x1:x2:x3.  How did you determine the best model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 output   R-squared:                       0.999\n",
      "Model:                            OLS   Adj. R-squared:                  0.999\n",
      "Method:                 Least Squares   F-statistic:                 4.941e+04\n",
      "Date:                Wed, 07 Oct 2015   Prob (F-statistic):          4.78e-300\n",
      "Time:                        22:29:07   Log-Likelihood:                -511.20\n",
      "No. Observations:                 214   AIC:                             1028.\n",
      "Df Residuals:                     211   BIC:                             1038.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.7927      0.068     11.574      0.000         0.658     0.928\n",
      "x2             0.2828      0.070      4.063      0.000         0.146     0.420\n",
      "x3            -0.7053      0.400     -1.765      0.079        -1.493     0.083\n",
      "==============================================================================\n",
      "Omnibus:                        0.960   Durbin-Watson:                   1.797\n",
      "Prob(Omnibus):                  0.619   Jarque-Bera (JB):                0.657\n",
      "Skew:                           0.099   Prob(JB):                        0.720\n",
      "Kurtosis:                       3.186   Cond. No.                         202.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 output   R-squared:                       0.999\n",
      "Model:                            OLS   Adj. R-squared:                  0.999\n",
      "Method:                 Least Squares   F-statistic:                 2.203e+04\n",
      "Date:                Wed, 07 Oct 2015   Prob (F-statistic):          1.60e-293\n",
      "Time:                        22:29:07   Log-Likelihood:                -504.93\n",
      "No. Observations:                 214   AIC:                             1024.\n",
      "Df Residuals:                     207   BIC:                             1047.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.8922      0.080     11.142      0.000         0.734     1.050\n",
      "x2             0.5174      0.099      5.251      0.000         0.323     0.712\n",
      "x3            56.4290    206.499      0.273      0.785      -350.682   463.540\n",
      "x1*x2         -0.0051      0.002     -3.058      0.003        -0.008    -0.002\n",
      "x1*x3         -0.7615      3.150     -0.242      0.809        -6.971     5.448\n",
      "x2*x3         -0.8851      3.222     -0.275      0.784        -7.238     5.468\n",
      "x1*x2*x3       0.0120      0.049      0.245      0.807        -0.085     0.109\n",
      "==============================================================================\n",
      "Omnibus:                        0.565   Durbin-Watson:                   1.806\n",
      "Prob(Omnibus):                  0.754   Jarque-Bera (JB):                0.309\n",
      "Skew:                           0.054   Prob(JB):                        0.857\n",
      "Kurtosis:                       3.151   Cond. No.                     4.91e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.91e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Read text file into variable data using panda\n",
    "# StudyNote: read_csv() if has no other args but file path, the read data when display will have \\t between columns\n",
    "# adding sep = '\\t' will solve the problem as data = pd.read_csv('regressionP1.txt', sep='\\t')\n",
    "# another way is to use delim_whitespace = true\n",
    "data = pd.read_csv('regressionP1.txt', delim_whitespace=True)\n",
    "# Extract the output column data into var y\n",
    "y = data['output']\n",
    "\n",
    "\n",
    "# ## scatter matrix\n",
    "# pd.scatter_matrix(data, alpha=0.2, figsize=(14,14), diagonal='kde')\n",
    "\n",
    "## create two way and three way interaction variable combinations\n",
    "data['x1*x2'] = data['x1'] * data['x2']\n",
    "data['x1*x3'] = data['x1'] * data['x3']\n",
    "data['x2*x3'] = data['x2'] * data['x3']\n",
    "data['x1*x2*x3'] = data['x1']*data['x2'] * data['x3']\n",
    "\n",
    "## model 1: all x\n",
    "x = data[['x1', 'x2', 'x3']]\n",
    "model = sm.OLS(y, x)\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "## model 2: all x and interactions\n",
    "x = data[['x1', 'x2', 'x3', 'x1*x2', 'x1*x3', 'x2*x3','x1*x2*x3']]\n",
    "model = sm.OLS(y, x)\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "\n",
    "## RESULT:\n",
    "## In both models, there are x1, x2, and x1*x2 that have significant coefficients. We can leave out x3\n",
    "## In the next step, we should try to compare model x1, x2, vs. x1, x2, x1*x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b\n",
    "Using the best model from 1a), analyze the regression assumptions.  This should include at least a histogram of the residuals, residuals as a function of the output, and a q-q plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2  Hierarchical Clustering (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of data has the following distance matrix.  Use the distance matrix approach to cluster this data by hand or using python calculations.  Produce the following output of the form for a hierarchical clustering using the Centroid based clustering. \n",
    "\n",
    "HigherCol     LowerCol     distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src='distmtx.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3  Gradient Search (15 pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3a  \n",
    "\n",
    "Use a gradient search approach to find the minimum value of:\n",
    "\n",
    "f(x,y,z) = (z-3x)4 exp(x-y) + (3x + y + 2z + 7)2exp(y-z)\n",
    "\n",
    "The gradient of the function is [dx dy dz] defined below. \n",
    "\n",
    "dx:\t -12(z-3x)3 exp(x-y) + (z-3x)4 exp(x-y) + 6(3x + y + 2z + 7)exp(y-z)\n",
    "\n",
    "dy:\t(z-3x) 4 exp(x-y)(-1) + 2(3x + y + 2z + 7)exp(y-z) + (3x+y+2z+7)2exp(y-z)\n",
    "\n",
    "dz:\t4(z-3x)3 exp(x-y) + 4(3x + y + 2z + 7)exp(y-z) - (3x + y + 2z + 7)2exp(y-z)\n",
    "\n",
    "Start at least with [x=-1 y=0 z=1] and explore different learning rates (alpha).  How sensitive is the gradient to the initial starting point?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3b\n",
    "\n",
    "An extremely common error in gradient searches is that either the gradient equation or the implementation are wrong.  A quick check is to look at the neighborhood of a given start point.  For the starting point, verify that a point f(x+deltax, y+deltay, z+deltaz) is lower than f(x,y,z) where the  deltax, deltay, and deltaz are small numbers in the appropriate direction of the calculated gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4  K-Modes (35 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4a  \n",
    "Write a k-modes algorithm.  We suggest that you find a k-means algorithm available pretty much everywhere and adapt it to perform k-modes for categorical data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4b  \n",
    "Cluster the attached sponge data using your k-modes algorithm.  Recall that the mode is the most frequently occurring pattern in categorical data.  Note that the first column of this data set is a set of labels, not data points.  To show the results, print a list of the output column labels on separate lines.  That is if you find 3 clusters (a,b,c), (d), (e,f), the output should look like\n",
    "\n",
    "a b c<br/>\n",
    "d<br/>\n",
    "e f<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4c\n",
    "Justify your choice of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 Gaussian Mixture Model (15 pts)\n",
    "\n",
    "Clustering using K-Means and Gaussian Mixture Models.  Cluster the gmmDataP5.txt file using 4 clusters for each.  You may need to try multiple different starting locations to get a stable result.   We recommend using the the scikit-learn k-Means and mixture GMM for this, although the PyMix from section would be equivalent and acceptable (but needs the older Python 2.7).   How do the results of the clustering compare?    Print the mean and covariances for the GMM, and the centroids for the k-means.\n",
    "Plotting the raw data points and ellipses for the GMM is useful but optional, but worth a few bonus points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 Bonus question:\n",
    "Write your own implementation of EM algorithm applied to the Gaussian Mixture Models.  Apply it to the same data set.  You do not have to generalize it to N dimensions—2 dimensions is sufficient for this problem if that is easier.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering acts sparse data with KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=37, n_init=1,\n",
      "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
      "    verbose=0)\n",
      "done in 0.015s\n",
      "[22 19 35 ...,  4 17 22]\n",
      "Clustering scene sparse data with KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=37, n_init=1,\n",
      "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
      "    verbose=0)\n",
      "[ 2  2 29 ...,  2  2 29]\n",
      "done in 0.043s\n"
     ]
    }
   ],
   "source": [
    "## Trying different feature extraction methods from sckitlearn and see if can be applied to HW4\n",
    "## More note can be found in the trial and test note.docx\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize the \"TfidVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool. http://scikit-learn.org/dev/modules/feature_extraction.html#text-feature-extraction\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "## These following two tools are also initialized, but can't be used report dtype error\n",
    "## TO-DO: need to fix it\n",
    "transformer = TfidfTransformer()\n",
    "hashvect = HashingVectorizer()\n",
    "\n",
    "#import file\n",
    "df = pd.read_csv('Data/all_plays_tokenized.txt',sep='\\t')\n",
    "\n",
    "# Get all the play names\n",
    "playnames = pd.unique(df.playname.ravel())\n",
    "\n",
    "# Array to hold bag of words for each play\n",
    "play_bagwords = []\n",
    "\n",
    "# Vectorizer results are normalized, which makes KMeans behave as\n",
    "# spherical k-means for better results. Since LSA/SVD results are\n",
    "# not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD()\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "## loop through all plays\n",
    "for i in range(len(playnames)):\n",
    "    \n",
    "    ### Create bag of words for all plays\n",
    "    p = df[df['playname'] == playnames[i]] # Get the sub data frame of each play\n",
    "    s = \"\" # Initiate empty string to hold bag of words for play\n",
    "\n",
    "    # Iterate all the rows to append the speech and speaker words to a string\n",
    "    for index,row in p.iterrows():\n",
    "        s += str(row['speaker'])\n",
    "        s += str(row['speech'])\n",
    "    \n",
    "    # Append the bag of words to each play\n",
    "    play_bagwords.append(s)\n",
    "    \n",
    "    ### Create bag of words for all the acts in each play\n",
    "    s = \"\" # Initiate empty string to hold bag of words for acts\n",
    "    acts = pd.unique(p.act.ravel()) # Get the number of acts and scenes\n",
    "    for j in range(len(acts)):\n",
    "        \n",
    "        a = p[p['act'] == acts[j]] #Get the current act\n",
    "        # Array to hold bag of words for each bag\n",
    "#         act_bagwords = []\n",
    "         # Iterate all the rows to append the speech and speaker words to a string\n",
    "        for index,row in a.iterrows():\n",
    "            s += str(row['speaker'])\n",
    "            s += str(row['speech'])\n",
    "        \n",
    "         # Append the bag of words to each act within the play\n",
    "        act_bagwords.append(s) \n",
    "        \n",
    "        s = \"\"\n",
    "        scenes = pd.unique(a.scene.ravel()) # Get the number of acts and scenes\n",
    "        # Array to hold bag of words for each scene\n",
    "#         scene_bagwords = []\n",
    "        for z in range(len(scenes)):\n",
    "        \n",
    "            sc = a[a['scene'] == scenes[z]] #Get the current act\n",
    "        \n",
    "             # Iterate all the rows to append the speech and speaker words to a string\n",
    "            for index,row in sc.iterrows():\n",
    "                s += str(row['speaker'])\n",
    "                s += str(row['speech'])\n",
    "    \n",
    "            # Append the bag of words to each act within the play\n",
    "            scene_bagwords.append(s)  \n",
    "        \n",
    "    #Vectorize analyze the similarities among plays\n",
    "    act_vect = vectorizer.fit_transform(act_bagwords)\n",
    "    scene_vect = vectorizer.fit_transform(scene_bagwords)\n",
    "    #analyze = vectorizer.build_analyzer()\n",
    "#     act_feature = act_vect.toarray() # Show the similarity array of plays\n",
    "#     scene_feature = scene_vect.toarray() # Show the similarity array of plays\n",
    "    act_feature = lsa.fit_transform(act_vect) # Apply Latent Semantic Analysis\n",
    "    scene_feature = lsa.fit_transform(scene_vect) # Apply Latent Semantic Analysis\n",
    "    #Write the scene and act feature array to text files for each play\n",
    "#     fn1 = str(playnames[i])+\"_Act_Features_Vectorize_LSA.txt\"\n",
    "#     fn2 = str(playnames[i])+\"_Scene_Features_Vectorize_LSA.txt\"\n",
    "#     np.savetxt(fn1, act_feature, delimiter=\",\")\n",
    "#     np.savetxt(fn2, scene_feature, delimiter=\",\")\n",
    "#     np.savetxt(\"Act_Features_Vectorize_LSA.txt\", act_feature, delimiter=\",\")\n",
    "#     np.savetxt(\"scene_Features_Vectorize_LSA.txt\", scene_feature, delimiter=\",\")\n",
    "#     explained_variance = svd.explained_variance_ratio_.sum()\n",
    "#     print(explained_variance)\n",
    "\n",
    "\n",
    "# Vectorize analyze the similarities among plays\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "play_vect = vectorizer.fit_transform(play_bagwords)\n",
    "#analyze = vectorizer.build_analyzer()\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "play_feature = play_vect.toarray() # Show the similarity array of plays\n",
    "\n",
    "play_feature = lsa.fit_transform(play_feature) # Apply Latent Semantic Analysis\n",
    "\n",
    "\n",
    "# explained_variance = svd.explained_variance_ratio_.sum()\n",
    "# print(explained_variance)\n",
    "\n",
    "\n",
    "# See the data arrary: print (play_feature.shape)\n",
    "\n",
    "#Write the plays feature array to a csv file\n",
    "# np.savetxt(\"Play_Features_Vectorize.txt\", play_feature, delimiter=\",\")\n",
    "# np.savetxt(\"Play_Features_Vectorize_LSA.txt\", play_feature, delimiter=\",\")\n",
    "\n",
    "# Take a look at the words in the vocabulary\n",
    "# vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# import numpy as np\n",
    "# print(vocab)\n",
    "\n",
    "### Print the count of each word in vocab -- not working somehow\n",
    "# # Sum up the counts of each vocabulary word\n",
    "# dist = np.sum(play_feature, axis=0)\n",
    "# # For each, print the vocabulary word and the number of times it \n",
    "# # appears in the training set\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print (count, tag)\n",
    "\n",
    "\n",
    "\n",
    "#Y = transformer.fit_transform(play_bagwords) -- To DO fix the error: no supported conversion for types: (dtype('<U98727')\n",
    "\n",
    "## Select act as you like it as a test to cluster different scenes\n",
    "# ayli = df[df['playname'] == 'the comedy of errors'] # Extract the as you like it play\n",
    "# # Extract different scenes\n",
    "# ayli_1 = ayli[ayli['scene']==1]\n",
    "# ayli_2 = ayli[ayli['scene']==2]\n",
    "# ayli_3 = ayli[ayli['scene']==3]\n",
    "# ayli_4 = ayli[ayli['scene']==4]\n",
    "# ayli_5 = ayli[ayli['scene']==5]\n",
    "# ayli_6 = ayli[ayli['scene']==6]\n",
    "# # Extract the speaker and speech from each scene\n",
    "# ayli_1_words = ayli_1[['speaker','speech']]\n",
    "# ayli_2_words = ayli_2[['speaker','speech']]\n",
    "# ayli_3_words = ayli_3[['speaker','speech']]\n",
    "# ayli_4_words = ayli_4[['speaker','speech']]\n",
    "# ayli_5_words = ayli_5[['speaker','speech']]\n",
    "# ayli_6_words = ayli_6[['speaker','speech']]\n",
    "# # Form each scene's speaker and speech note into one bag of words\n",
    "\n",
    "# # st1 = str(ayli_1_words['speaker'].apply(str) +' '+ ayli_1_words['speech'].apply(str))\n",
    "# st1 = \"\"\n",
    "# for index, row in ayli_1_words.iterrows():\n",
    "#     st1 += str(row['speaker'])\n",
    "#     st1 += str(row['speech'])\n",
    "# st2 = \"\"\n",
    "# for index, row in ayli_2_words.iterrows():\n",
    "#     st2 += str(row['speaker'])\n",
    "#     st2 += str(row['speech'])\n",
    "# st3 = \"\"\n",
    "# for index, row in ayli_3_words.iterrows():\n",
    "#     st3 += str(row['speaker'])\n",
    "#     st3 += str(row['speech'])\n",
    "# st4 = \"\"\n",
    "# for index, row in ayli_4_words.iterrows():\n",
    "#     st4 += str(row['speaker'])\n",
    "#     st4 += str(row['speech'])\n",
    "# st5 = \"\"\n",
    "# for index, row in ayli_5_words.iterrows():\n",
    "#     st5 += str(row['speaker'])\n",
    "#     st5 += str(row['speech'])\n",
    "# st6 = \"\"\n",
    "# for index, row in ayli_6_words.iterrows():\n",
    "#     st6 += str(row['speaker'])\n",
    "#     st6 += str(row['speech'])\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

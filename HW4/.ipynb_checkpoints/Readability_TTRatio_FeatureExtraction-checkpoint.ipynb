{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Extract type token ratio, average word length, average sentence length,Colemanâ€“Liau index  and other features \n",
    "## Refer to the Readability_Features folder, Column_Denote.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## tokenize, stem, remove stopwords\n",
    "def tokenStem(words):\n",
    "    words = words.strip('[').strip(']').lower() #remove brackets and lowercase\n",
    "    words = re.sub('[(){}<>\\'\"]', '', words)\n",
    "    stemmer = PorterStemmer()\n",
    "#     stops = stopwords.words('english')\n",
    "    output = [stemmer.stem(token) for token in wordpunct_tokenize(words) ] #stem words\n",
    "    return \"\".join(words) #merge into strings\n",
    "\n",
    "#import file\n",
    "df = pd.read_csv('Data/all_plays.txt',sep='\\t')\n",
    "# tokenize, stem, and remove stopwords\n",
    "df['speech'] = df['speech'].map(lambda x: tokenStem(x))\n",
    "\n",
    "# Get all the play names\n",
    "playnames = pd.unique(df.playname.ravel())\n",
    "\n",
    "# df['speech'] = df['speech'].map(lambda x: tokenStem(x))\n",
    "# # Array to hold bag of words for each play\n",
    "play_bagwords = []\n",
    "act_bagwords = []\n",
    "scene_bagwords = []\n",
    "\n",
    "## loop through all plays\n",
    "for i in range(len(playnames)):\n",
    "    \n",
    "    ### Create bag of words for all plays\n",
    "    p = df[df['playname'] == playnames[i]] # Get the sub data frame of each play\n",
    "    s = \"\" # Initiate empty string to hold bag of words for play\n",
    "\n",
    "    # Iterate all the rows to append the speech and speaker words to a string\n",
    "    for index,row in p.iterrows():\n",
    "        s += str(row['speaker'])\n",
    "        s += str(row['speech'])\n",
    "    \n",
    "    # Append the bag of words to each play\n",
    "    play_bagwords.append(s)\n",
    "   \n",
    "    ### Create bag of words for all the acts in each play\n",
    "    s = \"\" # Initiate empty string to hold bag of words for acts\n",
    "    acts = pd.unique(p.act.ravel()) # Get the number of acts and scenes\n",
    "    for j in range(len(acts)):\n",
    "        \n",
    "        a = p[p['act'] == acts[j]] #Get the current act\n",
    "        # Array to hold bag of words for each bag\n",
    "#         act_bagwords = []\n",
    "         # Iterate all the rows to append the speech and speaker words to a string\n",
    "        for index,row in a.iterrows():\n",
    "            s += str(row['speaker'])\n",
    "            s += str(row['speech'])\n",
    "        \n",
    "         # Append the bag of words to each act within the play\n",
    "        act_bagwords.append(s) \n",
    "        s = \"\"\n",
    "        scenes = pd.unique(a.scene.ravel()) # Get the number of acts and scenes\n",
    "        # Array to hold bag of words for each scene\n",
    "#         scene_bagwords = []\n",
    "        for z in range(len(scenes)):\n",
    "        \n",
    "            sc = a[a['scene'] == scenes[z]] #Get the current act\n",
    "        \n",
    "             # Iterate all the rows to append the speech and speaker words to a string\n",
    "            for index,row in sc.iterrows():\n",
    "                s += str(row['speaker'])\n",
    "                s += str(row['speech'])\n",
    "    \n",
    "            # Append the bag of words to each act within the play\n",
    "            scene_bagwords.append(s)  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Initiate arrays to hold stats about the works\n",
    "num_sentences = [] \n",
    "num_words = []\n",
    "num_letters = []\n",
    "avglen_words = []\n",
    "avglen_sentences = []\n",
    "CLI_score = []\n",
    "# count_less5letter_words = []\n",
    "\n",
    "\n",
    "# Loop through all scenes\n",
    "for x in scene_bagwords: \n",
    "    x = re.sub('[(){}<>\\'?,!:;\"]', '.', x) # Replace all signs with .\n",
    "    x = x.split('.') # Split into sentences\n",
    "    num_sentence = len(x) # Get the number of sentences for this play\n",
    "    num_word = 0\n",
    "    num_letter = 0\n",
    "    cli = 0\n",
    "    for y in x:\n",
    "        y = wordpunct_tokenize(y)\n",
    "        num_word += len((y)) # Accumulate the number of words for this sentence\n",
    "        for z in y:\n",
    "            num_letter += len(z) \n",
    "    cli = 5.88*num_letter/num_word - 29.6*num_sentence/num_word - 15.8\n",
    "    num_letters.insert(-1,num_letter) # Insert  num_letters into the array\n",
    "    num_words.insert(-1,num_word) # Insert  num_words into the array\n",
    "    num_sentences.insert(-1,num_sentence) # Insert  num_sentences into the array\n",
    "    avglen_words.insert(-1,num_letter/num_word)\n",
    "    avglen_sentences.insert(-1,num_word/num_sentence)\n",
    "    CLI_score.insert(-1,cli)\n",
    "stats_feature = [num_letters,num_words,num_sentences,CLI_score,avglen_words,avglen_sentences]\n",
    "# print(num_sentences,num_words,num_letters,CLI_score)\n",
    "stats_feature = np.array(stats_feature)\n",
    "stats_feature = stats_feature.T\n",
    "np.savetxt(\"Scene_style_feature.txt\", stats_feature, delimiter=\",\")\n",
    "\n",
    "#     # print(play_ttratio)\n",
    "# np.savetxt(\"Act_TTratio_.txt\", act_ttratio, delimiter=\",\")\n",
    "# # Split to each sentence\n",
    "# e = d.split('.')\n",
    "# # print(e)\n",
    "# print(e[1])\n",
    "# # Get the number of sentences in this corpos\n",
    "# print(len(e)-1)\n",
    "# ## Get the length(number of words) of the sentence 1\n",
    "# print(len(wordpunct_tokenize(e[1])))\n",
    "# ## Split the sentence into words\n",
    "# f = wordpunct_tokenize(e[1])\n",
    "# print (len(f[0]))\n",
    "# # f = wordpunct_tokenize(e[1])\n",
    "# # print(f)\n",
    "# # print(len(f[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "act_ttratio = []\n",
    "\n",
    "for x in act_bagwords:\n",
    "    total_words = len(wordpunct_tokenize(x))  # Get the number of words used\n",
    "    total_vocab = len(set(wordpunct_tokenize(x))) # Get the number of unique vocabs used\n",
    "    act_ttratio.insert(-1,total_vocab/total_words) # Insert the ttratio \n",
    "# print(play_ttratio)\n",
    "np.savetxt(\"Act_TTratio_.txt\", act_ttratio, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32183206106870227, 0.3335540254237288, 0.28943293267458153, 0.34536033914272257, 0.3243269894487723, 0.32161446443247194, 0.29911154985192495, 0.33028418099768264, 0.3273753527751646, 0.33730337078651684, 0.32380379175443874, 0.35039941902687, 0.3116771831424788, 0.34145325838017904, 0.3451912082327531, 0.2961689999161707, 0.30061543317026984, 0.2946845354615269, 0.3125292557341239, 0.32078204199855176, 0.29053335359367877, 0.28449299453811444, 0.2904374364191251, 0.3102593440122044, 0.28514318348698703, 0.28450218745857087, 0.3299655200064149, 0.31422986708365913, 0.3146608014296515, 0.3053809270111881, 0.30109042355908316, 0.3716646989374262, 0.31064487273318697, 0.2794516303926884, 0.3412844036697248, 0.29697603651578547, 0.3207441860465116]\n"
     ]
    }
   ],
   "source": [
    "## Array to hold the token type ratio, the higher the ratio, the richer the vocab is\n",
    "play_ttratio = []\n",
    "\n",
    "for x in play_bagwords:\n",
    "    total_words = len(wordpunct_tokenize(x))  # Get the number of words used\n",
    "    total_vocab = len(set(wordpunct_tokenize(x))) # Get the number of unique vocabs used\n",
    "    play_ttratio.insert(-1,total_vocab/total_words) # Insert the ttratio \n",
    "print(play_ttratio)\n",
    "np.savetxt(\"Play_TTratio_.txt\", play_ttratio, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
